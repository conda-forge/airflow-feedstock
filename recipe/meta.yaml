{% set name = "airflow" %}
{% set pypi_name = "apache-airflow" %}
{% set version = "2.8.4" %}

package:
  name: {{ name|lower }}-split
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ pypi_name[0] }}/{{ pypi_name }}/apache_airflow-{{ version }}.tar.gz
  sha256: 899dac9916ad2d973a0dd1b6c5c8d365d04bd2b8839f504d0f287e657365b77e

build:
  skip: true  # [win or py>=312]
  number: 0

requirements:
  build:
    - python                                 # [build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]
  host:
    - python
    - pip


outputs:
  - name: {{ name }}
    script: install_airflow.sh
    build:
      entry_points:
        - airflow=airflow.__main__:main
    requirements:
      build:
        - python                                 # [build_platform != target_platform]
        - cross-python_{{ target_platform }}     # [build_platform != target_platform]
      host:
        - python
        - gitdb ==4.0.11
        - gitpython ==3.1.42
        - hatchling ==1.22.3
        - packaging ==24.0
        - pathspec ==0.12.1
        - pluggy ==1.4.0
        # smmap 5.0.1 was not built on conda-forge
        - smmap ==5.0.0
        - tomli ==2.0.1  # [py<311]
        - trove-classifiers ==2024.3.3
        - pip
      run:
        - python
        - alembic >=1.13.1,<2.0
        - argcomplete >=1.10
        - asgiref
        - attrs >=22.1.0
        - blinker
        - colorlog >=4.0.2,<5.0
        - configupdater >=3.1.1
        - connexion >=2.10.0,<3.0

        # connexion[flask] dependencies
        - flask >=1.0.4,<3
        - itsdangerous >=0.24

        - cron-descriptor >=1.2.24
        - croniter >=2.0.2
        - cryptography >=0.9.3
        - deprecated >=1.2.13
        - dill >=0.2.2
        - flask-appbuilder ==4.3.11
        - flask-caching >=1.5.0
        - flask-login >=0.6.2
        # Extra constraint needed:
        # https://github.com/apache/airflow/pull/36895
        - flask-session >=0.4.0,<0.6
        - flask-wtf >=0.15
        - flask >=2.2,<2.3
        - fsspec >=2023.10.0
        - google-re2 >=1.0
        - gunicorn >=20.1.0
        - httpx
        - importlib-metadata >=1.7  # [py<39]
        - importlib-resources >=5.2,!=6.2.0,!=6.3.0,!=6.3.1  # [py<39]
        - itsdangerous >=2.0
        - jinja2 >=3.0.0
        - jsonschema >=4.18.0
        - lazy-object-proxy
        - linkify-it-py >=2.0.0
        - lockfile >=0.12.2
        - markdown-it-py >=2.1.0
        - markupsafe >=1.1.1
        - marshmallow-oneofschema >=2.0.1
        - mdit-py-plugins >=0.3.0
        - opentelemetry-api >=1.15.0
        - opentelemetry-exporter-otlp
        - packaging >=14.0
        - pathspec >=0.9.0
        - pendulum >=2.1.2,<4.0
        - pluggy >=1.0
        - psutil >=4.2.0
        - pygments >=2.0.1
        - pyjwt >=2.0.0
        - python-daemon >=3.0.0
        - python-dateutil >=2.3
        - python-nvd3 >=0.15.0
        - python-slugify >=5.0
        - requests >=2.27.0,<3
        - rfc3339-validator >=0.1.4
        - rich-argparse >=1.0.0
        - rich >=12.4.4
        - setproctitle >=1.1.8
        - sqlalchemy >=1.4.28,<2.0
        - sqlalchemy-jsonfield >=1.0
        - tabulate >=0.7.5
        - tenacity >=6.2.0,!=8.2.0
        - termcolor >=1.1.0
        - unicodecsv >=0.14.1
        - universal_pathlib >=0.1.4,<0.2.0
        - werkzeug >=2.0,<3
        # see https://github.com/apache/airflow/blob/{{ version }}/airflow_pre_installed_providers.txt
        - apache-airflow-providers-common-io
        - apache-airflow-providers-common-sql
        - apache-airflow-providers-ftp
        - apache-airflow-providers-http
        - apache-airflow-providers-imap
        - apache-airflow-providers-smtp
        - apache-airflow-providers-sqlite

    test:
      commands:
        - pip check
        - airflow --help
        # not working for linux emulation because passlib, a dependency of
        # sqlalchemy ends up creating a bcrypt hash, which Ubuntu CI's glibc
        # cannot handle.
        # https://github.com/conda-forge/airflow-feedstock/pull/103#issuecomment-1468162070
        - airflow db init  # [not linux-aarch64 and not linux-ppc64le]
      imports:
        - airflow
        - airflow.api
        - airflow.api.auth
        - airflow.api.auth.backend
        - airflow.api.client
        - airflow.api.common
        - airflow.api.common.experimental
        - airflow.api_connexion
        - airflow.cli
        - airflow.cli.commands
        - airflow.config_templates
        - airflow.contrib
        - airflow.contrib.hooks
        - airflow.contrib.operators
        - airflow.contrib.secrets
        - airflow.contrib.sensors
        - airflow.contrib.task_runner
        - airflow.contrib.utils
        - airflow.example_dags
        - airflow.example_dags.subdags
        - airflow.executors
        - airflow.hooks
        - airflow.jobs
        - airflow.kubernetes
        - airflow.lineage
        - airflow.macros
        - airflow.migrations
        - airflow.migrations.versions
        - airflow.models
        - airflow.operators
        - airflow.secrets
        - airflow.security
        - airflow.sensors
        - airflow.serialization
        - airflow.task
        - airflow.task.task_runner
        - airflow.ti_deps
        - airflow.ti_deps.deps
        - airflow.utils
        - airflow.utils.log
        - airflow.www
        - airflow.www.api
        - airflow.www.api.experimental
      requires:
        - pip

  # alternative name for the core package
  - name: apache-airflow
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
    test:
      imports:
        - airflow
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-apache-atlas
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - atlasclient >=0.1.2
    test:
      imports:
        - airflow.lineage
        - airflow.lineage.entities
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-apache-webhdfs
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - python-hdfs >=2.0.4
        # avro extra
        - fastavro >=0.21.19
        # kerberos extra
        - requests-kerberos >=0.7.0
        # dataframe extra
        - fastavro >=0.21.19
        - pandas >=0.14.1
    test:
      imports:
        - airflow
        - hdfs
        - airflow.hooks.webhdfs_hook
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-aiobotocore
    build:
      skip: true
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - aiobotocore >=2.7.0
    test:
      imports:
        - airflow
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-async
    build:
      skip: true
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - eventlet >=0.33.3
        - gevent >=0.13
        - greenlet >=0.4.9
    test:
      imports:
        - airflow
        - eventlet
        - gevent
        - greenlet
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-github_enterprise
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - authlib>=1.0.0
    test:
      imports:
        - airflow
        - authlib
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-google_auth
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - authlib>=1.0.0
    test:
      imports:
        - airflow
        - authlib
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-kerberos
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - pykerberos >=1.1.13
        - requests-kerberos >=0.10.0
        - thrift_sasl >=0.2.0
    test:
      imports:
        - airflow
        - kerberos
        # failing because of bcrypt issue in docker images, see:
        # https://github.com/conda-forge/airflow-feedstock/pull/103#issuecomment-1468162070
        - airflow.api.auth.backend.kerberos_auth  # [not aarch64 and not ppc64le]
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-ldap
    build:
      skip: True  # [arm64 or aarch64 or ppc64le]
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - ldap3 >=2.5.1
        - python-ldap
    test:
      imports:
        - airflow
        - ldap3
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-leveldb
    build:
      skip: True  # [arm64 or aarch64 or ppc64le]
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - plyvel
    test:
      imports:
        - airflow
        - plyvel
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-pandas
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - pandas >=1.2.5,<2.2
    test:
      imports:
        - airflow
        - pandas
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-password
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - bcrypt >=2.0.0
        - flask-bcrypt >=0.7.1
    test:
      imports:
        - airflow
        - flask_bcrypt
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-rabbitmq
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - amqp
    test:
      imports:
        - airflow
        - amqp
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-s3fs
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - s3fs >=2023.10.0
    test:
      imports:
        - airflow
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-saml
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - python3-saml >=1.16.0
    test:
      imports:
        - airflow
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-sentry
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - blinker >=1.1
        - sentry-sdk >=1.32.0,!=1.33.0
    test:
      imports:
        - airflow
        - sentry_sdk
        - airflow.sentry
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-statsd
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - statsd >=3.3.0
    test:
      imports:
        - airflow
        - statsd
      commands:
        - pip check
      requires:
        - pip

  - name: {{ name }}-with-virtualenv
    requirements:
      host:
        - python
      run:
        - python
        - {{ pin_subpackage(name, max_pin='x.x.x.x.x.x') }}
        - virtualenv
    test:
      imports:
        - airflow
        - virtualenv
      commands:
        - pip check
      requires:
        - pip

about:
  home: http://airflow.apache.org
  license: Apache-2.0
  license_file:
    - LICENSE
    - 3rd-party-licenses/LICENSE-bootstrap3-typeahead.txt
    - 3rd-party-licenses/LICENSE-bootstrap.txt
    - 3rd-party-licenses/LICENSE-d3js.txt
    - 3rd-party-licenses/LICENSE-d3-shape.txt
    - 3rd-party-licenses/LICENSE-d3-tip.txt
    - 3rd-party-licenses/LICENSE-dagre-d3.txt
    - 3rd-party-licenses/LICENSE-datatables.txt
    - 3rd-party-licenses/LICENSE-elasticmock.txt
    - 3rd-party-licenses/LICENSE-eonasdan-bootstrap-datetimepicker.txt
    - 3rd-party-licenses/LICENSE-flask-kerberos.txt
    - 3rd-party-licenses/LICENSE-hue.txt
    - 3rd-party-licenses/LICENSE-jqclock.txt
    - 3rd-party-licenses/LICENSE-jquery.txt
    - 3rd-party-licenses/LICENSE-moment.txt
    - 3rd-party-licenses/LICENSE-normalize.txt
    - 3rd-party-licenses/LICENSE-pytest-capture-warnings.txt
    - 3rd-party-licenses/LICENSE-reproducible.txt
    - 3rd-party-licenses/LICENSE-unicodecsv.txt


  summary: |
    Airflow is a platform to programmatically author, schedule and monitor
    workflows

  description: |
    Use airflow to author workflows as directed acyclic graphs (DAGs)
    of tasks. The airflow scheduler executes your tasks on an array of
    workers while following the specified dependencies. Rich command
    line utilities make performing complex surgeries on DAGs a snap.
    The rich user interface makes it easy to visualize pipelines
    running in production, monitor progress, and troubleshoot issues
    when needed.

    When workflows are defined as code, they become more maintainable,
    versionable, testable, and collaborative.

  doc_url: http://pythonhosted.org/airflow/profiling.html
  dev_url: https://github.com/apache/airflow

extra:
  recipe-maintainers:
    - sodre
    - halldc
    - xylar
